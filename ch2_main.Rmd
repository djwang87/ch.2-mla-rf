---
title: "chapter 2 - RF model"
author: "Dajun Wang"
date: "11/9/2019"
output:
  pdf_document:
    toc: no
    number_sections: yes
    fig_caption: yes
    df_print: kable
    highlight: tango
  html_document:
    toc: no
    df_print: paged
  word_document:
    toc: no
fontsize: 11pt
geometry: margin = 1.2in
bibliography: ../../PhD/zot-library.bib
editor_options:
  chunk_output_type: console
spacing: double
always_allow_html: true
mainfont: Times New Roman
header-includes:
- \setlength\parindent{24pt}
- \usepackage{indentfirst}
- \usepackage{setspace}\doublespacing
- \usepackage{lscape}
- \newcommand{\blandscape}{\begin{landscape}}
- \newcommand{\elandscape}{\end{landscape}}
---
```{r global options, cache=FALSE, include=FALSE}
set.seed(2807)
knitr::opts_chunk$set(fig.pos = 'H') #to set all images to top
knitr::read_chunk('ch2_main.Rmd')
options(tinytex.verbose = TRUE)
```

```{r setup, include=FALSE}
list.of.packages <- c("lubridate", "dplyr", "ggplot2","randomForest", "corrplot", "knitr", "glmm", "tinytex","xtable","ggcorrplot","stargazer","kableExtra", "captioner","formattable", "reshape2", "lme4", "e1071","zoo", "animalTrack", "janitor") 

new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if (length(new.packages)) install.packages(unlist(new.packages))
lapply(list.of.packages, require, character.only = T)

options(tibble.print_max = Inf) # To show all rows
options(tibble.width = Inf) # To show all columns; Inf controls value

knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
                      echo=FALSE, warning=FALSE, message=FALSE)
```


```{r captioner, include = FALSE}
table_nums = captioner(prefix = 'Table')
table_nums(name = 'dogtask', caption = "Description of the behavioural tasks performed by kennel club-trained domestic dogs.")
table_nums(name = "summstats", caption = "Predictor variables and their statistic labels used for predicting dog behavioural tasks in the random forest models.")
table_nums(name = "fft.train", caption = "The prediction results of the Random Forest model constructed from the Fast-fourier transformation (FFT) dataset.")
table_nums(name = "raw.train", caption = "The prediction results of the Random Forest model constructed from the raw acceleration measurement (RAW) dataset.")
table_nums(name = "tt.compare", caption = "A comparison of the prediction results of the training and testing datasets from the Random Forest models constructed from the Fast-fourier transformation (FFT) and raw acceleration measurement (RAW) dataset.")
table_nums(name = "groupcoarse", caption = "A comparison between the grouped and coarse-level predictions from the training and testing dataset made by Random Forest models constructed from Fast-fourier transformation (FFT) datasets.")
table_nums(name = "accthrtable", caption = "The mean, minimum and maximum variance values of acceleromenter measurements for the movement behaviours of dogs.")

fig_nums <- captioner()
fig_nums(name = 'mtry', caption = "The prediction accuracies of the Random Forests models constructed with a range of mTry values mirroring the number of predictor variables used. The red and blue lines represent the Random Forest models constructed with the Fast-fourier transformation (FFT) and raw acceleration measurement (RAW) datasets respectively.")
fig_nums(name = 'varimp', caption = "The variable importance of the the different predictor variables used in the Random Forest models. Higher node purity represents greater importance in the model's prediction capabilities. The red and blue bars represent the Random Forest models constructed with the Fast-fourier transformation (FFT) and raw acceleration measurement (RAW) datasets respectively.")
fig_nums(name = 'oob', caption = "The Out-Of-Bag error estimates of the Random Forest model plotted against the number of trees (*n*) used in making predictions. The red and blue dots represent the Random Forest models constructed with the Fast-fourier transformation (FFT) and raw acceleration measurement (RAW) datasets respectively.")
fig_nums(name = "accthrplot", caption = "The sampling of GPS re-locations based with an accelerometer threshold of 10,000 in the Z axis. The red, green and blue lines represents variance in acceleration measurements collected from the X, Y and Z axis respectively.")
```

# Introduction

# Material and methods

## Random forest model construction

Random Forests (RF) is a relatively novel and powerful machine learning algorithm that has been reported to work well with complex ecological data that cannot be easily fitted with traditional methods such as generalized linear models. Through the use of tri-axial accelerometery measurements (i.e., acceleration values), RF models can also be used to predict unobservable behaviours in wild, free-ranging animals based on information collected from examining captive animals. To do this, the RF model must first be trained to recognize and predict labelled behaviours in a 'training' dataset with decision trees constructed with suitable predictor variables (`r table_nums('summstats', display = 'cite')`) used for classifying and predicting behaviours (`r table_nums('dogtask', display = "cite")`). Subsequently, the accuracy of the constructed RF model is then assessed by having the model predict on a 'testing' dataset.

For the classification and labelling of known behaviours in accelerometery data, I collared 11 healthy adult dogs of three different dog breeds: German shepherds (n = 9, six males and three females, age range: 1–8 years, mean age: 5 years old, Rottweiler (n = 1, male, age: 8 years old) and Golden Retriever (n = 1, male, age: 1 year old) with an accelerometer-equipped wildlife tracking collar (Type 1C-heavy, E-obs GmbH; Grünwald, Germany). The wildlife tracking collar used in this chapter is also used for collaring and tracking free-roaming dogs in the subsequent chapters. All eleven dogs were well-trained individuals that could perform the selected repertoire of movement behaviours (`r table_nums('dogtask', display = "cite")`) solely with the verbal commands of their trainer while being off-leash. 

`r table_nums('dogtask')`
```{r table dogtasks}
tabl.dogtasks = data.frame(
  Behaviour = c(rep("Eat", 1), rep("Stationary", 3), rep("Slow", 2), rep("Run", 1)),
  Behaviour = c("Eat", "Stand", "Lie", "Sit", "Walk", "Forage", "Run"),
  Description = c("Individual under studying is eating or drinking from a bowl whilst standing. No visible locomotion but the posture of the head is angled approximately 45° downwards.",
                  "Individual under study is standing on all four limbs with no visible locomotion. Resting posture of the dog's head will vary between individuals.",
                  "Individuals under study is lying prone on the ground with all limps extended out. For some individuals, the dog may be lying fully on its side. In all footages, there was no visible movement.",
                  "Individual under study is sitting on its haunches with no visible movement. Similar to 'Stand', head posture varies between individuals but body posture remains similar.",
                  "Individual under study is walking freely off-leash with no visible acceleration.",
                  "Similar to 'Walk' but the position of the dog's head is angled downwards as it searches for treats on the ground.",
                  "Individual under study is moving significantly faster than 'Walk' (e.g., trotting, running). The dog was tasked to chase after a tossed ball, and the task is considered completed once the tossed ball has been received by the dog."),
  check.names = FALSE
)

kable(tabl.dogtasks, "html", booktabs = T) %>%
  kable_styling(position = "center") %>%
  column_spec(1:2, bold = T) %>%
  collapse_rows(columns = 1:2, latex_hline = "major", valign = "middle")
  column_spec(2, width = "30em")

```

For the purpose of collecting data to train the RF models (i.e., training dataset), the wildlife tracking collar was programmed to sample continuously at 10 hz, and the collar was mounted such that the x-, y-, and z- axes of the tri-axial acclerometer were parallel to the median (surge), the dorsal (sway), and the dorsal (heave) planes of the animal, respectively. The design and alignment of the collar provided the tri-axial acceleromenter the capacity to measure the surge (back and forth movement on the x-axis), sway (left and right motion on the y-axis) and heave (up and down on the z-axis) motion of the animal. Data was collected in October 2016 and May 2019 in a grassy field to a) simulate the environment where the intended free-roaming dogs for study are typically found, and b) provide sufficient ground for the dogs to move continuously. The terrain selected for this component was relatively flat to avoid compromising the gravitional forces acting on the heave axis.

`r table_nums('summstats')`
```{r table summstats}
tabl.summstats = data.frame(Labels = c("Mean", "Min", "Max", "Kurtosis", "Skewness", 
                                       "Range", "Standard deviation", "Pitch", "Roll",
                                       "Overall Dynamic Body Acceleration"),
                            Variables = c("mean.x, mean.y, mean.z", "min.x, min.y, minz",
                                          "max.x, max.y, max.z",
                                          "kurt.x, kurt.y, kurt.z", "skew.x, skew.y, skew.z", "range.x, range.y, range.z", "sd.x. sd.y, sd.z", "pitch", "roll", "ODBA"),
                            Description = c("The calculated mean of the acceleration measurements within each burst for axes x, y and z.",
                                            "The smallest acceleration measurement within each burst for axes x, y and z.",
                                            "The largest acceleration measurement within each burst for axes x, y and z.",
                                            "The relative flatness of the acceleration measurements within each burst for axes x, y and z.",
                                            "The relative skewness of the acceleration measurements within each burst for axes x, y and z.",
                                            "The calculated difference between the largest and smallest acceleration measurements of axes x, y, and z.",
                                            "The calculated standard deviations of the acceleration measurements of axes x, y, and z.",
                                            "The calculated ratio between axes x, y, and z.",
                                            "The calculated angle between axes y and z.",
                                            "The sum of the dynamic acceleration values of axes x, y, and z."))


kable(tabl.summstats, "latex", longtable = T, booktabs = T) %>%
    kable_styling(position = "center", latex_options = c("repeat_header")) %>%
  column_spec(1, bold = T, width = "10em") %>%
  column_spec(2, width = "8em") %>%
  column_spec(3, width = "20em")
```

 To compare between the capabilities of the RF model in predicting fine-scale and coarse-scale behaviours, the dogs were commanded to perform 'fine-scale' behavioural tasks (`r table_nums('dogtask', display = "cite")`) continuously for at least 2 min in a stipulated sequence except for Forage, Eat and Run as these tasks are either energetically taxing for the dogs or behaviourally inconsistent. Each dog was tasked to perform the entire repertoire of behaviours at least twice (with verbal commands from their trainer), and approximately 300 minutes of video footages were obtained. For the collection of 'coarse-scale' behaviours, footages containing behaviours from the earlier component were re-categorized accordingly (`r table_nums('dogtask', display = "cite")`).
 
 The collected accelerometery data was viewed with an acceleration viewer (http://www.movebank.org, version 33) and the time-stamps of the collected accelerometer measurements were synchronized and labelled manually to the time-stamps of the video footages of each behavioural task. The labelled acceleration measurements were binned into two-second windows (or 'bursts') which accomodates at least two full strides of movement-based dog behaviours (i.e., walking, foraging) while reducing the influence of un-intentional behavioural transitions.

To examine the accuracies of the RF models in predicting fine-scale movement behaviours, two different RF models were constructed based on the type of acceleration measurements used. The first RF model, henceforth termed as the RAW model, was constructed with a series of summary statistics (`r table_nums('summstats', display = 'cite')`) applied onto each burst of raw labelled acceleration measurements from all three axes. Similarly, the second RF model, henceforth termed as the FFT model, was constructed by transforming the labelled acceleration measurement with a Fast-fourier transformation prior to applying the same series of summary statistics. The summary statistics used in the transformation of the bursts of acceleration measurements are used to characterize the acceleration forces from the different behavioural tasks, and describe the predictor variables used in both RF models.

In addition to the aforementioned derivation of the predictor variables, I also derived the overall dynamic body acceleration (ODBA) of each burst by summing the absolute dynamic body acceleration values of each axis. The dynamic body acceleration was obtained by removing the static acceleration from each burst of acceleration measurement. This was done by subtracting a running mean (of 2s; burst lenght) from the collected acceleration measurement in each burst [@gleissMakingOverallDynamic2011].

The `randomForest` package [@liawClassificationRegressionRandomForest2002] in `R` was  used to construct the RF models used for predicting two different scale of dog behaviours; fine- and coarse-scale predictions. For both scales of prediction, the datasets used were subsampled at a 80% proportion as a 'training' dataset to prepare the RF model, while the remaining 20% was used as a 'testing' dataset. The default number of trees (*n* = 500) were used in the RF model construction as the Out-of-Bag (OOB) error estimates, and the RF models were cross-validated by comparing the prediction accuracies of RF models constructed (*n* = 24; `r fig_nums("mtry", display = 'cite')`) with a range of mTry values mirroring the number of described predictor variables (`r table_nums('summstats', display = 'cite')`). 

In the prediction of fine-scale dog behaviours, I compared prediction accuracies of each individual dog behaviour (`r table_nums(name = 'dogtask', display = 'cite')`) between two RF models constructed with the FFT and RAW dataset. Prediction accuracies derived from the 'training' dataset and 'testing' dataset were tabulated.



With both datasets (i.e., FFT and RAW), the RF models were prepared by subsampling 80% of the transformed dataset as a 'training' dataset, while the remaining 20% was reserved as a 'testing' dataset. For both RF models (i.e., FFT and RAW), 

For both RF models, the labelled datasets (FFT and RAW) were subsampled at a 80% proportion as a training dataset to prepare the RF model, while the remaining 20% was used as a testing and validation dataset. The default number of trees (*n* = 500) were used in the RF model construction as the Out-of-Bag (OOB) error estimates, a method of measuring prediction error rates, was found to be consistently stable (`r fig_nums('oob', display = 'cite')`). To refine and improve the RF model's predictive capacity, I cross validated the RF models by comparing the prediction accuracies of RF models constructed (*n* = 21; `r fig_nums("mtry", display = 'cite')`) with a range of mTry values mirroring the number of described predictor variables (`r table_nums('summstats', display = 'cite')`). 



Following which, the best predictive RF model was constructed and the classification error rate and mean prediction accuracy of each behavioural task in the model were tallied and tabled (`r table_nums("trainpred", display = 'cite')`).

## Accelerometer threshold accelerometer-informed GPS sampling

The wildlife tracking collar is equipped with the capacity to sample GPS relocations dynamically based on the accelerometer measurements collected in real-time. With the use of a selected accelerometer threshold (ACT), reseachers are able to selectively intensify the sampling of GPS re-locations only when the animal is moving (hence higher variance in accelerometer readings). This feature not only reduces the energy and memory consumption during the research period, it also allows the researcher to identify movement paths or patterns that are ecologically interesting (e.g., predation or harrassement events).

For this function to work realistically, I used the above constructed RF model to identify the variances of the predicted movement behaviours and selected a variance value (`r table_nums('accthrtable', display = 'cite')`) for the axis that best represents movement or motion in a dog (i.e., Z axis; the acceleration in the heave axis typified by the rolling gait in dogs). Following which, I ground truth the selected variance by walking four dogs (Mongrel breeds, 4 females, age range 2-7 years old) with the wildlife tracking collar equipped. All dogs were walked continuously for approximately ten minutes, and with the studied dog resting (with the collar on) only in the beginning and end of each walk. The dogs were rested in a non-motion movement behaviour (e.g., lying, sitting or standing) to simulate the resting behaviour of free-roaming dogs in the wild. The GPS and ACC dataset were then retrieved from the collar, and the occurence of quick- and long-burst gps relocations were plotted sequentially against the variance of the accelerometer measurements collected from all three axes (`r fig_nums("accthrplot", display = "cite")`). 

```{r fine-scale data prep, include = FALSE}
train.data = read.csv("2020-08_10_mla-train-data.txt") # latest train set, incl. new dogs
train.data$X.1 = NULL

# Preparing the summ stats of the model
fine.data = train.data %>%
  group_by(dog, behaviour, index) %>%
  mutate(ra.x = rollmean(X, 20), # rolling mean for smoothing parameter
         ra.y = rollmean(Y, 20),
         ra.z = rollmean(Z, 20))%>%
  mutate(dy.X = X - ra.x, # to attain dynamic body acceleration to be used for ODBA
         dy.Y = Y - ra.y,
         dy.Z = Z - ra.z) %>%
  mutate(abs.X = abs(dy.X), # absolute acceleration for ODBA
         abs.Y = abs(dy.Y),
         abs.Z = abs(dy.Z),
         pitch = animalTrack::pitch(dy.X, dy.Y, dy.Z), # finding pitch raw accel
         roll = animalTrack::roll(dy.Y, dy.Z)) %>% # finding roll raw accel
  mutate(fft.X = Re(fft(X)), # FFT dataset
         fft.Y = Re(fft(Y)),
         fft.Z = Re(fft(Z))) %>%
  mutate(fft.pitch = animalTrack::pitch(fft.X, fft.Y, fft.Z),
         fft.roll = animalTrack::roll(fft.Y, fft.Z))


fine.raw = fine.data %>% # raw accel dataset
  group_by(dog, behaviour, index) %>%
  summarize(behaviour = first(behaviour),
            mean.X = mean(X, na.rm = TRUE),
            mean.Y = mean(Y, na.rm =TRUE),
            mean.Z = mean(Z, na.rm = TRUE),
            min.X = min(X, na.rm = TRUE),
            min.Y = min(Y, na.rm =TRUE),
            min.Z = min(Z, na.rm = TRUE),
            kurt.X = kurtosis(X, na.rm = TRUE),
            kurt.Y = kurtosis(Y, na.rm =TRUE),
            kurt.Z = kurtosis(Z, na.rm = TRUE),
            skew.X = skewness(X, na.rm = TRUE),
            skew.Y = skewness(Y, na.rm =TRUE),
            skew.Z = skewness(Z, na.rm = TRUE),
            max.X = max(X, na.rm = TRUE),
            max.Y = max(Y, na.rm =TRUE),
            max.Z = max(Z, na.rm = TRUE),
            sd.X = sd(X, na.rm = TRUE),
            sd.Y = sd(Y, na.rm =TRUE),
            sd.Z = sd(Z, na.rm = TRUE),
            range.X = max.X - min.X,
            range.Y = max.Y - min.Y,
            range.Z = max.Z - min.Z,
            pitch = mean(pitch),
            roll = mean(roll),
            odba = sum(abs.X, abs.Y, abs.Z)) %>%
  mutate(type = "fine.raw")

fine.fft = fine.data %>% # fft dataset
  group_by(dog, behaviour, index) %>%
  summarize(behaviour = first(behaviour),
            mean.X = mean(fft.X, na.rm = TRUE),
            mean.Y = mean(fft.Y, na.rm =TRUE),
            mean.Z = mean(fft.Z, na.rm = TRUE),
            min.X = min(fft.X, na.rm = TRUE),
            min.Y = min(fft.Y, na.rm =TRUE),
            min.Z = min(fft.Z, na.rm = TRUE),
            kurt.X = kurtosis(fft.X, na.rm = TRUE),
            kurt.Y = kurtosis(fft.Y, na.rm =TRUE),
            kurt.Z = kurtosis(fft.Z, na.rm = TRUE),
            skew.X = skewness(fft.X, na.rm = TRUE),
            skew.Y = skewness(fft.Y, na.rm =TRUE),
            skew.Z = skewness(fft.Z, na.rm = TRUE),
            max.X = max(fft.X, na.rm = TRUE),
            max.Y = max(fft.Y, na.rm =TRUE),
            max.Z = max(fft.Z, na.rm = TRUE),
            sd.X = sd(fft.X, na.rm = TRUE),
            sd.Y = sd(fft.Y, na.rm =TRUE),
            sd.Z = sd(fft.Z, na.rm = TRUE),
            range.X = max.X - min.X,
            range.Y = max.Y - min.Y,
            range.Z = max.Z - min.Z,
            pitch = mean(fft.pitch),
            roll = mean(fft.roll),
            odba = sum(abs.X, abs.Y, abs.Z)) %>%
    mutate(type = "fine.fft")

# train.data -> fine.data -> fine.raw + fine.fft

```

```{r coarse scale data prep, include = FALSE}
coarse.data = train.data
coarse.data <- coarse.data %>%
     mutate(behaviour = recode(behaviour, 
                               eat = "Eat",
                               forage = "Slow",
                               walk = "Slow",
                               run = "Fast",
                               sit = "Stationary",
                               stand = "Stationary",
                               lie = "Stationary"))
coarse.data$index = rep(1:(nrow(coarse.data)/20), each = 20)

coarse.data = coarse.data %>%
  group_by(dog, behaviour, index) %>%
  mutate(ra.x = rollmean(X, 20), # rolling mean for smoothing parameter
         ra.y = rollmean(Y, 20),
         ra.z = rollmean(Z, 20)) %>%
  mutate(dy.X = X - ra.x, # to attain dynamic body acceleration to be used for ODBA
         dy.Y = Y - ra.y,
         dy.Z = Z - ra.z) %>%
  mutate(abs.X = abs(dy.X), # absolute acceleration for ODBA
         abs.Y = abs(dy.Y),
         abs.Z = abs(dy.Z),
         pitch = pitch(dy.X, dy.Y, dy.Z), # finding pitch raw accel
         roll = roll(dy.Y, dy.Z)) %>% # finding roll raw accel
  mutate(fft.X = Re(fft(X)), # FFT dataset
         fft.Y = Re(fft(Y)),
         fft.Z = Re(fft(Z))) %>%
  mutate(fft.pitch = pitch(fft.X, fft.Y, fft.Z),
         fft.roll = roll(fft.Y, fft.Z))

coarse.raw = coarse.data %>%
  group_by(dog, behaviour, index) %>%
  summarize(behaviour = first(behaviour),
            mean.X = mean(X, na.rm = TRUE),
            mean.Y = mean(Y, na.rm =TRUE),
            mean.Z = mean(Z, na.rm = TRUE),
            min.X = min(X, na.rm = TRUE),
            min.Y = min(Y, na.rm =TRUE),
            min.Z = min(Z, na.rm = TRUE),
            kurt.X = kurtosis(X, na.rm = TRUE),
            kurt.Y = kurtosis(Y, na.rm =TRUE),
            kurt.Z = kurtosis(Z, na.rm = TRUE),
            skew.X = skewness(X, na.rm = TRUE),
            skew.Y = skewness(Y, na.rm =TRUE),
            skew.Z = skewness(Z, na.rm = TRUE),
            max.X = max(X, na.rm = TRUE),
            max.Y = max(Y, na.rm =TRUE),
            max.Z = max(Z, na.rm = TRUE),
            sd.X = sd(X, na.rm = TRUE),
            sd.Y = sd(Y, na.rm =TRUE),
            sd.Z = sd(Z, na.rm = TRUE),
            range.X = max.X - min.X,
            range.Y = max.Y - min.Y,
            range.Z = max.Z - min.Z,
            pitch = mean(pitch),
            roll = mean(roll),
            odba = sum(abs.X, abs.Y, abs.Z)) %>%
    mutate(type = "coarse.raw")

coarse.fft = coarse.data %>%
  group_by(dog, behaviour, index) %>%
  summarize(behaviour = first(behaviour),
            mean.X = mean(fft.X, na.rm = TRUE),
            mean.Y = mean(fft.Y, na.rm =TRUE),
            mean.Z = mean(fft.Z, na.rm = TRUE),
            min.X = min(fft.X, na.rm = TRUE),
            min.Y = min(fft.Y, na.rm =TRUE),
            min.Z = min(fft.Z, na.rm = TRUE),
            kurt.X = kurtosis(fft.X, na.rm = TRUE),
            kurt.Y = kurtosis(fft.Y, na.rm =TRUE),
            kurt.Z = kurtosis(fft.Z, na.rm = TRUE),
            skew.X = skewness(fft.X, na.rm = TRUE),
            skew.Y = skewness(fft.Y, na.rm =TRUE),
            skew.Z = skewness(fft.Z, na.rm = TRUE),
            max.X = max(fft.X, na.rm = TRUE),
            max.Y = max(fft.Y, na.rm =TRUE),
            max.Z = max(fft.Z, na.rm = TRUE),
            sd.X = sd(fft.X, na.rm = TRUE),
            sd.Y = sd(fft.Y, na.rm =TRUE),
            sd.Z = sd(fft.Z, na.rm = TRUE),
            range.X = max.X - min.X,
            range.Y = max.Y - min.Y,
            range.Z = max.Z - min.Z,
            pitch = mean(fft.pitch),
            roll = mean(fft.roll),
            odba = sum(abs.X, abs.Y, abs.Z)) %>%
    mutate(type = "coarse.fft")

#train.data -> coarse.data -> coarse.fft + raw.fft
rf.data = rbind(fine.fft, fine.raw, coarse.fft, coarse.raw)
#write.csv(rf.data, "2020-08-10_cleaned-coarse-data.txt")
```

```{r rf model dataset preparation}
rf.data = read.csv("2020-08-10_cleaned-coarse-data.txt")
rf.data$X = NULL
rf.data$index = as.factor(row_number(rf.data$behaviour))
rf.data$type = as.factor(rf.data$type)

rf.data <- rf.data[order(runif(nrow(rf.data))),] # re-ordering the rows of the df

# RAW.fine training dts
fine.raw_train <- rf.data %>% 
  filter(type == "fine.raw") %>%
  group_by(behaviour) %>%
  slice(seq(n()*.8)) #  take 80% of each behaviour classification

fine.raw_test = rf.data %>%
  filter(type == "fine.raw") %>%
  dplyr::filter(!index %in% fine.raw_train$index) # filter out rows that are NOT IN train dts


# RAW.coarse training dts
coarse.raw_train <- rf.data %>% 
  filter(type == "coarse.raw") %>%
  group_by(behaviour) %>%
  slice(seq(n()*.8)) #  take 80% of each behaviour classification

coarse.raw_test = rf.data %>%
  filter(type == "coarse.raw") %>%
  dplyr::filter(!index %in% coarse.raw_train$index) # filter out rows that are NOT IN train dts


# FFT.fine training dts
fine.fft_train <- rf.data %>% 
  filter(type == "fine.fft") %>%
  group_by(behaviour) %>%
  slice(seq(n()*.8)) #  take 80% of each behaviour classification

fine.fft_test = rf.data %>%
  filter(type == "fine.fft") %>%
  dplyr::filter(!index %in% fine.fft_train$index) # filter out rows that are NOT IN train dts


# FFT.coarse training dts
coarse.fft_train <- rf.data %>% 
  filter(type == "coarse.fft") %>%
  group_by(behaviour) %>%
  slice(seq(n()*.8)) #  take 80% of each behaviour classification

coarse.fft_test = rf.data %>%
  filter(type == "coarse.fft") %>%
  dplyr::filter(!index %in% coarse.fft_train$index) # filter out rows that are NOT IN train dts

```

# Results 

For the purpose of building the tranining dataset of the RF model, I collected more than 400,000 raw acceleration measurements (per axis) and approximately 400 minutes of video footage from 11 kennel-trained domestic dogs. From the collected video footages, seven different dog behaviours were identified which included sitting, standing, lying, eating, walking, foraging, and running (`r table_nums('dogtask', display = 'cite')`). After identifying and cleaning the collected accleration measurements, approximately 3000 bursts of labelled acceleration data comprising of nearly 1,260,000 transformed acceleration measurements (from the three axes) were used to construct and train the RF model.

```{r mtry dataset prep, cache = TRUE}
set.seed(2807)
## Fine-scale raw mtry loop
fine.raw_train$behaviour = droplevels(fine.raw_train$behaviour)
fine.raw_train$index = NULL
fine.raw_train$dog = NULL
fine.raw_train$type = NULL

fine.raw_test$behaviour = droplevels(fine.raw_test$behaviour)
fine.raw_test$index = NULL
fine.raw_test$dog = NULL
fine.raw_test$type = NULL

model.mtry = c() # Do 'mtry' loop to identify best value for mtry
i=24
for (i in 1:24) {
  model.mtry.train <- randomForest(behaviour ~ ., data = fine.raw_train, ntree = 500, mtry = i, importance = TRUE)
  predTest.mtry <- predict(model.mtry.train, fine.raw_test, type = "class")
  model.mtry[i] = mean(predTest.mtry == fine.raw_test$behaviour)
}

fine.raw.mtry = data.frame(mTry = c(1:24), Accuracy = c(model.mtry))
fine.raw.mtry$Accuracy = percent(fine.raw.mtry$Accuracy)
fine.raw.mtry$type = "fine.raw"

raw.mtry = fine.raw.mtry %>%
  arrange(desc(Accuracy)) %>%
  slice(1)

## Fine-scale FFT mtry loop
fine.fft_train$behaviour = droplevels(fine.fft_train$behaviour)
fine.fft_train$index = NULL
fine.fft_train$dog = NULL
fine.fft_train$type = NULL

fine.fft_test$behaviour = droplevels(fine.fft_test$behaviour)
fine.fft_test$index = NULL
fine.fft_test$dog = NULL
fine.fft_test$type = NULL

model.mtry = c() # Do 'mtry' loop to identify best value for mtry
i=24
for (i in 1:24) {
  model.mtry.train <- randomForest(behaviour ~ ., data = fine.fft_train, ntree = 500, mtry = i, importance = TRUE)
  predTest.mtry <- predict(model.mtry.train, fine.fft_test, type = "class")
  model.mtry[i] = mean(predTest.mtry == fine.fft_test$behaviour)
}

fine.fft.mtry = data.frame(mTry = c(1:24), Accuracy = c(model.mtry))
fine.fft.mtry$Accuracy = percent(fine.fft.mtry$Accuracy)
fine.fft.mtry$type = "fine.fft"

fft.mtry = fine.fft.mtry %>%
  arrange(desc(Accuracy)) %>%
  slice(1)

## coarse-scale FFT mtry loop
coarse.fft_train$behaviour = droplevels(coarse.fft_train$behaviour)
coarse.fft_train$index = NULL
coarse.fft_train$dog = NULL
coarse.fft_train$type = NULL

coarse.fft_test$behaviour = droplevels(coarse.fft_test$behaviour)
coarse.fft_test$index = NULL
coarse.fft_test$dog = NULL
coarse.fft_test$type = NULL

model.mtry = c() # Do 'mtry' loop to identify best value for mtry
i=24
for (i in 1:24) {
  model.mtry.train <- randomForest(behaviour ~ ., data = coarse.fft_train, ntree = 500, mtry = i, importance = TRUE)
  predTest.mtry <- predict(model.mtry.train, coarse.fft_test, type = "class")
  model.mtry[i] = mean(predTest.mtry == coarse.fft_test$behaviour)
}

coarse.fft.mtry = data.frame(mTry = c(1:24), Accuracy = c(model.mtry))
coarse.fft.mtry$Accuracy = percent(coarse.fft.mtry$Accuracy)
coarse.fft.mtry$type = "coarse.fft"

coarse.mtry = coarse.fft.mtry %>%
  arrange(desc(Accuracy)) %>%
  slice(1)
```
```{r fine-scale mtry ggplot}
fine.scale.mtry = rbind(fine.fft.mtry, fine.raw.mtry)
fine.scale.mtry$type = as.factor(fine.scale.mtry$type)

(ggplot(data = fine.scale.mtry, aes(x = mTry, y = Accuracy, color = type)) +
    geom_line() +
    geom_point(size = 3) +
    scale_y_continuous(labels = scales::percent) + 
    scale_color_manual(values = c("red", "blue")) +    # Red = fft, blue = raw
    xlab("Number of predictor variables used (mTry value)") + 
    ylab("Prediction accuracy") + 
    theme_minimal() + 
    theme(legend.position = "none")

)

```
`r fig_nums('mtry')`

From `r fig_nums('mtry', display = 'cite')`, it was found that the constructed RF model had the highest prediction accuracy when only four predictor variables were available or used when describing the differences between each movement behaviour. At present, the results from `r fig_nums('mtry', display = 'cite')` found that the difference in selecting between the best (i.e., 4) and worst (i.e., 1 or 20) performing mTry parameter resulted in an approximate 1% difference in mean prediction accuracy which suggests that the current RF model predictive capabilities, in spite of the large number of predictor variables used, is relatively consistent.
```{r rf model construction}
set.seed(2807)
fft.model = randomForest(behaviour ~., data = fine.fft_train, 
                        method = "class", ntree = 500, mtry = as.integer(fft.mtry[1]))
fft.predTest = predict(fft.model, fine.fft_test, type = "class")

raw.model = randomForest(behaviour ~., data = fine.raw_train, 
                        method = "class", ntree = 500, mtry = as.integer(raw.mtry[1]))
raw.predTest = predict(raw.model, fine.raw_test, type = "class")

coarse.model = randomForest(behaviour ~., data = coarse.fft_train, 
                        method = "class", ntree = 500, mtry = as.integer(coarse.mtry[1]))
coarse.predTest = predict(coarse.model, coarse.fft_test, type = "class")

```

`r fig_nums('varimp', display = 'cite')` demonstrates the mean decrease in Gini index in the predictor variables used for decision-making in the RF model. Node purity (as represented by the Gini index) refers to how well the trees split the data (i.e., decision-making) and predictor variables with a higher decrease in Gini index represents a a purer node that makes has greater importance in the RF model predictive and decision making capacity. In any case, the removal of the top-most predictor variable (hence, highest decrease in Gini index) will result in a RF model with poorer prediction accuracies. 

``` {r variable importance prep, include = FALSE} 
f.imp = varImpPlot(fft.model, main = "FFT")
f.imp = as.data.frame(f.imp)
f.imp$type = "FFT"
f.imp$varnames = rownames(f.imp)
r.imp = varImpPlot(raw.model, main = "RAW")
r.imp = as.data.frame(r.imp)
r.imp$type = "Raw"
r.imp$varnames = rownames(r.imp)
imp.plot = rbind(f.imp, r.imp)
imp.plot$type = as.factor(imp.plot$type)
ip = melt(imp.plot, varnames = 'varnames')
```

```{r variable importance ggplot}
(ggplot(ip, aes(x = reorder(varnames, value), y = value, fill = type))+
    geom_bar(stat = 'identity', position = 'dodge') +
    scale_fill_manual(values = c("red", "blue")) + 
    ylab("Node purity") + 
    xlab("Predictor variables") + 
    theme_classic() + 
    theme(legend.position = "none")
  
)
#Importance of each variable for behaviour prediction accuracy
```
`r fig_nums('varimp')`

Using the selected mTry value as reference (`r fig_nums('mtry', display = 'cite')`, mTry value of 4), it is suggestive that the Y axis may have a larger role in differentiating between the different movement behaviours and postures as the important predictor variables used in the RF model included a higher number of statistical transformations (i.e., 2 out of 4) derived from the Y axis (i.e., standard deviation of Y; sd.y, and range of Y; range.y). In addition, the ten most predictor variables in `r fig_nums('mtry', display = 'cite')` also found the statistical transformations of the Y axis were more prevalent than those from the X and Z axis (e.g., 40%, 30% and 30%; Y, X and Z respectively).

With regards to predictive stability in the cosntructed RF model, `r fig_nums('oob')` found that the Out-of-Bag error rates remained relatively stable after 100 trees were grown. Despite using 500 trees to develop the RF model, the average run-time for predicting on the testing dataset (remaining 20% of the subsampled labelled acceleration dataset; approximately 600 bursts of accleration measurements) took approximately two minutes. As such, increasing the number of trees used in the RF model will not significantly improve its predictive capacity, hence the trade-off between run-time and predictive power may not be feasible when larger datasets are used in the subsequent chapters.

```{r OOB error ggplot}
# plot(fft.model$err.rate[,1], ylab = "Out-Of-Bag Error Rate estimate", xlab = "Number of trees", main = "fft dataset")
oob.df = data.frame(index = c(1:500), 
                    FFT = fft.model$err.rate[,1], 
                    Raw = raw.model$err.rate[,1])

(ggplot(data = oob.df, aes(x = index)) +
    xlab("Number of trees") +
    ylab("Out-of-bag Error rate") +
    geom_point(aes(y = FFT), color = "red") + # Red = FFT
    geom_point(aes(y = Raw), color = 'blue') + # blue = Raw
    scale_y_continuous(labels = scales::percent) +
    theme_classic()
  )
```
`r fig_nums('oob')`


`r table_nums('fft.train')`
``` {r FFT training table}
fft.results = 1-fft.model$confusion[,8] # fft fine-scale train set
fft.model.df = data.frame(Behaviour = c("Eat", "Forage", "Lie", "Run", "Sit", "Stand", "Walk"),
                             Eat = as.integer(fft.model$confusion[,1]),
                             Forage = as.integer(fft.model$confusion[,2]),
                             Lie = as.integer(fft.model$confusion[,3]),
                             Run = as.integer(fft.model$confusion[,4]),
                             Sit = as.integer(fft.model$confusion[,5]),
                             Stand = as.integer(fft.model$confusion[,6]),
                             Walk = as.integer(fft.model$confusion[,7]),
                             check.names = FALSE)

fft.model.df = fft.model.df %>%
  adorn_totals('row')%>%
  adorn_percentages()%>%
  adorn_pct_formatting(digits = 2)
fft.model.df = fft.model.df[1:7,]
kable(fft.model.df, 'latex', booktabs = T)
```

`r table_nums('raw.train')`
```{r Raw training table}
raw.model.df = data.frame(Behaviour = c("Eat", "Forage", "Lie", "Run", "Sit", "Stand", "Walk"),
                             Eat = as.integer(raw.model$confusion[,1]),
                             Forage = as.integer(raw.model$confusion[,2]),
                             Lie = as.integer(raw.model$confusion[,3]),
                             Run = as.integer(raw.model$confusion[,4]),
                             Sit = as.integer(raw.model$confusion[,5]),
                             Stand = as.integer(raw.model$confusion[,6]),
                             Walk = as.integer(raw.model$confusion[,7]),
                             check.names = FALSE)

raw.model.df = raw.model.df %>%
  adorn_totals('row')%>%
  adorn_percentages()%>%
  adorn_pct_formatting(digits = 2)
raw.model.df = raw.model.df[1:7,]
kable(raw.model.df, "latex", booktabs = T)
# fft.raw.indi.comparison = data.frame(Behaviour = c("Eat", "Forage", 
#                                                    "Lie", "Run", "Sit", "Stand", "Walk"),
#                               FFT = fft.model.df[1:7,2],
#                               Raw = raw.model.df[1:7,2],
#                               FFT = fft.model.df[1:7,3],
#                               Raw = raw.model.df[1:7,3],
#                               FFT = fft.model.df[1:7,4],
#                               Raw = raw.model.df[1:7,4],
#                               FFT = fft.model.df[1:7,5],
#                               Raw = raw.model.df[1:7,5],
#                               FFT = fft.model.df[1:7,6],
#                               Raw = raw.model.df[1:7,6],
#                               FFT = fft.model.df[1:7,7],
#                               Raw = raw.model.df[1:7,7],
#                               FFT = fft.model.df[1:7,8],
#                               Raw = raw.model.df[1:7,8],
#                               check.names = FALSE)
```

```{r testing data prep}
fft.table = table(fine.fft_test$behaviour, fft.predTest) # FFT fine-scale test set
fft.table = as.data.frame.matrix(fft.table) # 

accuracy = vector()
for (i in 1:nrow(fft.table)) {
  accuracy[i] = fft.table[i,i]/sum(fft.table[i,])
}

fft.table = cbind(fft.table, accuracy)
fft.table = as.data.frame(fft.table)
fft.table$accuracy = percent(fft.table$accuracy)

raw.table = table(fine.raw_test$behaviour, raw.predTest) # raw fine-scale test set
raw.table = as.data.frame.matrix(raw.table)  

accuracy = vector()
for (i in 1:nrow(raw.table)) {
  accuracy[i] = raw.table[i,i]/sum(raw.table[i,])
}

raw.table = cbind(raw.table, accuracy) 
raw.table = as.data.frame(raw.table)
raw.table$accuracy = percent(raw.table$accuracy)
```

`r table_nums('tt.compare')`
```{r fft and raw comparison table}
fft.model.df$accuracy = percent(c(fft.model.df[1,2], fft.model.df[2,3],
                                  fft.model.df[3,4],fft.model.df[4,5], fft.model.df[5,6],
                                  fft.model.df[6,7], fft.model.df[7,8]))

raw.model.df$accuracy = percent(c(raw.model.df[1,2], raw.model.df[2,3],
                                  raw.model.df[3,4], raw.model.df[4,5], raw.model.df[5,6], raw.model.df[6,7],
                                  raw.model.df[7,8]))

fft.raw.comparison = data.frame(Behaviour = c("Eat", "Forage", "Lie", "Run", "Sit", "Stand", "Walk"),
                                FFT = fft.model.df$accuracy,
                                Raw = raw.model.df$accuracy,
                                Improvement = c(fft.model.df$accuracy- raw.model.df$accuracy),
                                FFT = fft.table$accuracy,
                                Raw = raw.table$accuracy,
                                Improvement = c(fft.table$accuracy-raw.table$accuracy),
                                check.names = FALSE)


fft.raw.comparison %>%
  kable(format = "latex", booktabs = T) %>%
  kable_styling(position = 'center') %>%
  add_header_above(c(" " = 1, "Training datasets" = 3, "Testing datasets" = 3),
                   bold = T, italic = T)
```

Looking at the RF model predictive capabilities on the training dataset (i.e., subsampled 80% of the labelled acceleration dataset), the RF model was found to predict movement-based behaviours (e.g., 'Run', 'Walk' and 'Forage') more accurately than non-movement based behaviours (e.g., 'Lie', 'Sit', and 'Stand'). For example, 'Run' and 'Walk' had comparatively higher prediction accuracies (`r raw.model.df[4,8]` and `r raw.model.df[7,8]`, respectively) whereas 'Sit', 'Stand', and 'Walk' were mis-classified twice as much (approximately `r 1-mean(raw.model.df[c(3,5,6),8])`). The poor predictive capabilities on non-movement behaviours was likely attributed to the the RF model inability to differentiate between the different postures held during non-movement behaviour tasks. For example, 'Lie' and 'Sit' often mis-classified interchangeably (e.g., 'Sit' as 'Lie', ; and 'Lie' as 'Sit' while the mis-classification for 'Stand' were evenly distributed between 'Lie', 'Sit' and 'Walk'

Not surprisingly, the RF model had the lowest prediction rates for the 'Forage' and 'Eat' behaviours (`r raw.model.df[1,8]` and `r raw.model.df[2,8]`, respectively). As discussed previously, it is likely that the RF model was unable to accurately predict 'Forage' and 'Eat' accurately as these behaviours bear strong postural and movement resemblance to 'Walk' and 'Lie' (e.g.,  and, respectively). For example, the 'Eat' behaviour represents the dog in-study consuming or drinking from a bowl and the these dogs often assume a lying position in the duration of this behavioural task. Likewise, the 'Forage' behaviour can be difficult for the RF model to differentiate from the 'Walk' behaviour as the behavioural task for 'Forage' was represented by the dog in-study walking with its head angled downwards (`r fig_nums('dogtask', display = 'cite')`).

```{r grouped comparison prep}
raw.model.df = data.frame(Behaviour = c("Eat", "Forage", "Lie", "Run", "Sit", "Stand", "Walk"),
                             Eat = as.integer(raw.model$confusion[,1]),
                             Forage = as.integer(raw.model$confusion[,2]),
                             Lie = as.integer(raw.model$confusion[,3]),
                             Run = as.integer(raw.model$confusion[,4]),
                             Sit = as.integer(raw.model$confusion[,5]),
                             Stand = as.integer(raw.model$confusion[,6]),
                             Walk = as.integer(raw.model$confusion[,7]),
                             check.names = FALSE)

raw.model.df = raw.model.df %>%
  adorn_totals('row')%>%
  adorn_percentages()%>%
  adorn_pct_formatting(digits = 2)
raw.model.df = raw.model.df[1:7,]

coarse.model.df = data.frame(Behaviour = c("Eat", "Fast", "Slow", "Stationary"),
                             Eat = as.integer(coarse.model$confusion[,1]),
                             Fast = as.integer(coarse.model$confusion[,2]),
                             Slow = as.integer(coarse.model$confusion[,3]),
                             Stationary = as.integer(coarse.model$confusion[,4]))

coarse.model.df = coarse.model.df %>%
  adorn_totals('row')%>%
  adorn_percentages()%>%
  adorn_pct_formatting(digits = 2)
coarse.model.df = coarse.model.df[1:4,]
coarse.model.df$accuracy = percent(c(coarse.model.df[1,2], coarse.model.df[2,3],
                             coarse.model.df[3,4], coarse.model.df[4,5]))

#### coarse-scale test dataset
coarse.table = table(coarse.fft_test$behaviour, coarse.predTest)
coarse.table = as.data.frame.matrix(coarse.table)

accuracy = vector()
for (i in 1:nrow(coarse.table)) {
  accuracy[i] = coarse.table[i,i]/sum(coarse.table[i,])
}

coarse.table = cbind(coarse.table, accuracy)
coarse.table = as.data.frame(coarse.table)
coarse.table$accuracy = percent(coarse.table$accuracy)

#### fine-scale grouped training set
stationary.predict = percent(sum(fft.model$confusion[3,3], fft.model$confusion[5,5], fft.model$confusion[6,6])/sum(fft.model$confusion[3,1:7], fft.model$confusion[5,1:7], fft.model$confusion[6,1:7]))

slow.predict = percent(sum(fft.model$confusion[2,2] + fft.model$confusion[7,7])/sum(fft.model$confusion[2,1:7] + fft.model$confusion[7,1:7]))

fast.predict = percent(1-(fft.model$confusion[4,8]))
eat.predict = percent(1-(fft.model$confusion[1,8]))

#### fine-scale grouped testing set
stationary.test = percent(sum(fft.table[3,3] + fft.table[5,5] + fft.table[6,6])/sum(fft.table[3,1:7] + fft.table[5,1:7] + fft.table[6,1:7]))

slow.test = percent(sum(fft.table[2,2] + fft.table[7,7])/sum(fft.table[2,1:7] + fft.table[7,1:7]))

fast.test = fft.table[4,8]
eat.test = fft.table[1,8]
```

`r table_nums('groupcoarse')`
```{r grouped comparison table}
grouped.comparison = data.frame(Behaviour = c("Eat", "Fast", "Slow", "Stationary"),
                             Grouped = c(eat.predict,
                                         fast.predict,
                                         slow.predict,
                                        stationary.predict),
                             Coarse = coarse.model.df$accuracy,
                             Improvement = c(eat.predict,fast.predict,slow.predict,stationary.predict)-coarse.model.df$accuracy,
                             Grouped = c(eat.test, 
                                        fast.test, 
                                        slow.test,
                                        stationary.test),
                             Coarse = coarse.table$accuracy,
                             Improvement = c(eat.test, fast.test, slow.test,stationary.test)-coarse.table$accuracy,
                             check.names = FALSE)
grouped.comparison %>%
  kable(format = "latex", booktabs = T) %>%
  kable_styling(latex_options = 'basic', position = 'center') %>%
  add_header_above(c(" " = 1, " Training datasets" = 3, "Testing datasets" = 3),
                   bold = T, italic = T)

```
As with most RF models, overfitting is a common occurence when the constructed RF model is tasked to predict on the remaining subsampled testing dataset. Even though overall prediction accuracy between the training and testing datasets differed slightly (`r percent(1528/2072)` and insert_cp , respectively), the prediction accuracies of individual behaviours in the testing dataset may be significantly worse (`r table_nums('testpred', display = 'cite')`). For example, the mis-classification rates for 'Eat' and 'Forage' increased by blank and blank respectively yet the prediction accuracies for 'Lie' and 'Sit' improved slightly (< 10%). Nevertheless, it is likely that these discrepancies in the prediction accuracies of individual behaviours were caused by the reduced sampling size of the testing dataset as the overall prediction accuracy for both datasets were still found to be comparable.

```{r reading acc-thr data}
df = read.csv("2020-07-20_mla-train-data.csv")
df$X = NULL
df$dog = as.factor("newguys")
data = df %>%
  mutate(behaviour = annotation,
         X = acceleration.x,
         Y = acceleration.y,
         Z = acceleration.z) %>%
  select(dog, sample, burst.timestamp, sample.timestamp, X, Y, Z, behaviour)
df2 = read.csv("acc_randomforest-training-data03.csv")
df = rbind(data, df2)
df = tbl_df(df)
df$behaviour = as.factor(df$behaviour)

walk = subset(df, df$behaviour == "walk")
run = subset(df, df$behaviour == "run")
lie = subset(df, df$behaviour == "lie")
sit = subset(df, df$behaviour == "sit")
stand = subset(df, df$behaviour == "stand")
forage = subset(df, df$behaviour == "forage")
eat = subset(df, df$behaviour == "eat")

move = rbind(walk, run, forage)
still = rbind (lie, sit, stand)

```

```{r acc-thr prep}
walk = walk[1:(round(nrow(walk)/26)*26),]
walk$index = rep(1:(nrow(walk)/26), each = 26)

walk.var = walk %>%
  group_by(dog,index) %>%
  summarise(var.x = var(X),
            var.y = var(Y),
            var.z = var(Z)) %>%
  mutate(behaviour = "walk")

run = run[1:(round(nrow(run)/26)*26),]
run$index = rep(1:(nrow(run)/26), each = 26)

run.var = run %>%
  group_by(dog,index) %>%
  summarise(var.x = var(X),
            var.y = var(Y),
            var.z = var(Z)) %>%
  mutate(behaviour = "run")

forage = forage[1:(round(nrow(forage)/26)*26),]
forage$index = rep(1:(nrow(forage)/26), each = 26)

forage.var = forage %>%
  group_by(dog,index) %>%
  summarise(var.x = var(X),
            var.y = var(Y),
            var.z = var(Z)) %>%
  mutate(behaviour = "forage")

lie = lie[1:(round(nrow(lie)/26)*26),]
lie$index = rep(1:(nrow(lie)/26), each = 26)

lie.var = lie %>%
  group_by(dog,index) %>%
  summarise(var.x = var(X),
            var.y = var(Y),
            var.z = var(Z)) %>%
  mutate(behaviour = "lie")


sit = sit[1:(round(nrow(sit)/26)*26),]
sit$index = rep(1:(nrow(sit)/26), each = 26)

sit.var = sit %>%
  group_by(dog,index) %>%
  summarise(var.x = var(X),
            var.y = var(Y),
            var.z = var(Z)) %>%
  mutate(behaviour = "sit")


stand = stand[1:(round(nrow(stand)/26)*26),]
stand$index = rep(1:(nrow(stand)/26), each = 26)

stand.var = stand %>%
  group_by(dog,index) %>%
  summarise(var.x = var(X),
            var.y = var(Y),
            var.z = var(Z)) %>%
  mutate(behaviour = "stand")

eat = eat[1:(round(nrow(eat)/26)*26),]
eat$index = rep(1:(nrow(eat)/26), each = 26)

eat.var = eat %>%
  group_by(dog,index) %>%
  summarise(var.x = var(X),
            var.y = var(Y),
            var.z = var(Z)) %>%
  mutate(behaviour = "eat")

move = rbind(walk.var, run.var, forage.var)
move = na.omit(move)
move$behavior = as.factor(move$behaviour)


still = rbind(eat.var, lie.var, sit.var, stand.var)
still = na.omit(still)
still$behaviour = as.factor(still$behaviour)

df.all = rbind(eat.var, run.var, forage.var, walk.var, stand.var, sit.var, lie.var)
df.all = na.omit(df.all)
df.all$behaviour = as.factor(df.all$behaviour)
```

`r table_nums("accthrtable")`
```{r acc-thr}
mean.variance = df.all %>% # The greatest difference between variance values are in axis Z
  group_by(behaviour) %>%
  summarise(mean.X = mean(var.x),
            mean.Y = mean(var.y),
            mean.Z = mean(var.z)) %>%
  arrange(mean.Z)

median.variance = df.all %>% # The greatest difference between variance values are in axis Z
  group_by(behaviour) %>%
  summarise(median.X = median(var.x),
            median.Y = median(var.y),
            median.Z = median(var.z)) %>%
  arrange(median.Z)


all.variance.table = data.frame(Activity = c(rep("Resting", 4), rep("Movement", 3)),
                                Behaviour = c("Lie", "Eat", "Sit", "Stand", "Forage", "Walk", "Run"),
                                X = mean.variance$mean.X,
                                Y = mean.variance$mean.Y,
                                Z = mean.variance$mean.Z,
                                X = median.variance$median.X,
                                Y = median.variance$median.Y,
                                Z = median.variance$median.Z,
                                check.names = F)

all.variance.table %>%
  kable(format = "latex", booktabs = T, align = 'c') %>%
  kable_styling(latex_options = 'scale_down', position = "center") %>%
  column_spec(1, bold = T) %>%
  collapse_rows(columns = 1, latex_hline = "major", valign = "middle") %>%
  add_header_above(c(" " = 2, "Mean" = 3, "Median" = 3))
```

Using the variances calculated from the labelled acceleration measurements (`r table_nums("accthrtable", display = 'cite')`), I sampled a range of variance values between 'Stand' and 'Forage' across the three axes to identify a variance threshold value (VTV) that could consistently initiate the collection of short-bursts GPS re-locations in the wildlife tracking collar. To do this, I used the `rle()` function from base `R` to examine and count the frequency of which a feasible VTV (from the range of feasible variance values) is met or exceeded by the variance of movement-based labelled acceleration measurements across the three axes. The VTV that determines the highest frequency of met or exceeded variance values was then used to program the wildlife tracking collar for the dog walking experiment.

```{r acc-thr ggplot}
match = read.csv('2020-07-21_acc-thr_gps-cleaned2.csv') # manual-cleaned gps data
match$burst = match$X
match$time.diff4 = as.factor(match$time.diff3)

match$minutes = match$sampled.time/60

(ggplot(match, aes(x = minutes)) +
  geom_line(aes(y = var.x), color = 'red') + 
  geom_line(aes(y = var.y), color = 'green') + 
  geom_line(aes(y = var.z), color = 'blue') + 
  geom_point(aes(y = as.numeric(time.diff3*50000)), size = 2) + 
  scale_y_continuous("Variance in acceleration", sec.axis = sec_axis(~ . / 50000, name = "Presence of quick burst GPS")) + 
  labs( x = "Sampling duration [min]") + 
  theme_classic()
)
# Set acc-thr as decided in above section in the gps collar, and 
# took 4 different dogs for a walk with a planned route.
```
`r fig_nums("accthrplot")`

With the VTV identified and programmed into the collar, `r fig_nums("accthrplot", display = "cite")` found that the collection short-burst GPS re-locations was most consistent when a VTV of 10,000 is programmed onto the Z-axis. For example, the collection of short-term GPS re-location is only triggered if the variance of collected acceleration measurements from the Z axis exceeds 10,000 repeatedly. That being said, movement is typically associated with acceleration in the surge axis (X-axis; red line) yet the heave axis (Z axis) appears to bemore sensitive to the up-and-down acceleration forces exerted by rolling gait of a walking dog. Seemingly, having access to the ground truthed information on the acceleration forces of a moving dog allows one to program a suitable VTV to examine biologically and ecologically interesting movement patterns in the subsequent chapters.

# Discussion


- no real need to prune the RF model even with 21 p.v as analysis ran relatively quick and n trees used also showed consistency.
- average run time was less than a minute

# Conclusion

\newpage

# References
# Supplementary codes

```{r  rf prediction bar plot , eval = FALSE, include = FALSE}
model.df = data.frame(Behaviour = c(model$classes),
                  Error = c(model$confusion[,8]))
# 
# model.corr = data.frame(Behaviour = c(model$classes),
#                   eat = c(model$confusion[,1]),
#                   forage = c(model$confusion[,2]),
#                   lie = c(model$confusion[,3]),
#                   run = c(model$confusion[,4]),
#                   sit = c(model$confusion[,5]),
#                   stand = c(model$confusion[,6]),
#                   walk = c(model$confusion[,7]))
# 
# model.matrix = as.matrix(model.corr[,-1], nrow = 7, ncol = 7)
# model.prop.t = round(prop.table(model.matrix,1),3)
# model.corr.plot = corrplot(model.prop.t,
#                             type = "full",
#                             order = "hclust",
#                             method = "number",
#                             tl.col = "black",
#                             tl.srt = 45)
#Correlation plot of the prediction accuracy of behaviours within the training dataset
# A repeat of information, feels slightly unnecessary


model.individual.behaviour.plot = ggplot(data = model.df, 
                                          aes(x= reorder(Behaviour, +Error), y=Error)) + geom_bar(stat = "identity")

model.individual.behaviour.plot = model.individual.behaviour.plot + coord_flip()

model.individual.behaviour.plot 
```



```{r psd dataset prep, eval=FALSE, include=FALSE}

psd.x = rep(0,(length(eat$X)/2))
psd.y = rep(0,(length(eat$Y)/2))
psd.z = rep(0,(length(eat$Z)/2))

for (i in 1:(length(eat$X)/20)) {
  a = i*20-19
  b = i*20
  c = i*10-9
  d = i*10
  test1 = psdcore(eat$X[a:b])
  test2 = psdcore(eat$Y[a:b])
  test3 = psdcore(eat$Z[a:b])
  psd.x[c:d] = test1$spec
  psd.y[c:d] = test2$spec
  psd.z[c:d] = test3$spec
}

psd.eat = as.data.frame(cbind(psd.x, psd.y, psd.z))
psd.eat$behaviour = "eat"

psd.x = rep(0,(length(forage$X)/2))
psd.y = rep(0,(length(forage$Y)/2))
psd.z = rep(0,(length(forage$Z)/2))

for (i in 1:(length(forage$X)/20)) {

  a = i*20-19
  b = i*20
  c = i*10-9
  d = i*10
  test1 = psdcore(forage$X[a:b])
  test2 = psdcore(forage$Y[a:b])
  test3 = psdcore(forage$Z[a:b])
  psd.x[c:d] = test1$spec
  psd.y[c:d] = test2$spec
  psd.z[c:d] = test3$spec
}

psd.forage = as.data.frame(cbind(psd.x, psd.y, psd.z))
psd.forage$behaviour = "forage"


psd.x = rep(0,(length(lie$X)/2))
psd.y = rep(0,(length(lie$Y)/2))
psd.z = rep(0,(length(lie$Z)/2))

for (i in 1:(length(lie$X)/20)) {

  a = i*20-19
  b = i*20
  c = i*10-9
  d = i*10
  test1 = psdcore(lie$X[a:b])
  test2 = psdcore(lie$Y[a:b])
  test3 = psdcore(lie$Z[a:b])
  psd.x[c:d] = test1$spec
  psd.y[c:d] = test2$spec
  psd.z[c:d] = test3$spec
}

psd.lie = as.data.frame(cbind(psd.x, psd.y, psd.z))
psd.lie$behaviour = "lie"


psd.x = rep(0,(length(run$X)/2))
psd.y = rep(0,(length(run$Y)/2))
psd.z = rep(0,(length(run$Z)/2))

for (i in 1:(length(run$X)/20)) {

  a = i*20-19
  b = i*20
  c = i*10-9
  d = i*10
  test1 = psdcore(run$X[a:b])
  test2 = psdcore(run$Y[a:b])
  test3 = psdcore(run$Z[a:b])
  psd.x[c:d] = test1$spec
  psd.y[c:d] = test2$spec
  psd.z[c:d] = test3$spec
}

psd.run = as.data.frame(cbind(psd.x, psd.y, psd.z))
psd.run$behaviour = "run"


psd.x = rep(0,(length(sit$X)/2))
psd.y = rep(0,(length(sit$Y)/2))
psd.z = rep(0,(length(sit$Z)/2))

for (i in 1:(length(sit$X)/20)) {

  a = i*20-19
  b = i*20
  c = i*10-9
  d = i*10
  test1 = psdcore(sit$X[a:b])
  test2 = psdcore(sit$Y[a:b])
  test3 = psdcore(sit$Z[a:b])
  psd.x[c:d] = test1$spec
  psd.y[c:d] = test2$spec
  psd.z[c:d] = test3$spec
}

psd.sit = as.data.frame(cbind(psd.x, psd.y, psd.z))
psd.sit$behaviour = "sit"


psd.x = rep(0,(length(stand$X)/2))
psd.y = rep(0,(length(stand$Y)/2))
psd.z = rep(0,(length(stand$Z)/2))

for (i in 1:(length(stand$X)/20)) {

  a = i*20-19
  b = i*20
  c = i*10-9
  d = i*10
  test1 = psdcore(stand$X[a:b])
  test2 = psdcore(stand$Y[a:b])
  test3 = psdcore(stand$Z[a:b])
  psd.x[c:d] = test1$spec
  psd.y[c:d] = test2$spec
  psd.z[c:d] = test3$spec
}

psd.stand = as.data.frame(cbind(psd.x, psd.y, psd.z))
psd.stand$behaviour = "stand"


psd.x = rep(0,(length(walk$X)/2))
psd.y = rep(0,(length(walk$Y)/2))
psd.z = rep(0,(length(walk$Z)/2))

for (i in 1:(length(walk$X)/20)) {

  a = i*20-19
  b = i*20
  c = i*10-9
  d = i*10
  test1 = psdcore(walk$X[a:b])
  test2 = psdcore(walk$Y[a:b])
  test3 = psdcore(walk$Z[a:b])
  psd.x[c:d] = test1$spec
  psd.y[c:d] = test2$spec
  psd.z[c:d] = test3$spec
}

psd.walk = as.data.frame(cbind(psd.x, psd.y, psd.z))
psd.walk$behaviour = "walk"

psd.two.sec = rbind(psd.eat, psd.forage, psd.lie, psd.run, psd.sit, 
                    psd.stand, psd.walk)
psd.two.sec$behaviour = as.factor(psd.two.sec$behaviour)
psd.two.sec$burst = rep(1:(nrow(psd.two.sec)/10), each = 10)
psd.two.sec$index = rep(1:10, length = (nrow(psd.two.sec)/10))
psd.two.sec = psd.two.sec %>%
  rename(X = psd.x,
         Y = psd.y,
         Z = psd.z)

psd.data = psd.two.sec %>%
  group_by(behaviour, burst) %>%
  summarize(behaviour = first(behaviour),
            mean.x = mean(X, na.rm = TRUE),
            mean.y = mean(Y, na.rm =TRUE),
            mean.z = mean(Z, na.rm = TRUE),
            min.x = min(X, na.rm = TRUE),
            min.y = min(Y, na.rm =TRUE),
            min.z = min(Z, na.rm = TRUE),
            kurt.x = kurtosis(X, na.rm = TRUE),
            kurt.y = kurtosis(Y, na.rm =TRUE),
            kurt.z = kurtosis(Z, na.rm = TRUE),
            skew.x = skewness(X, na.rm = TRUE),
            skew.y = skewness(Y, na.rm =TRUE),
            skew.z = skewness(Z, na.rm = TRUE),
            max.x = max(X, na.rm = TRUE),
            max.y = max(Y, na.rm =TRUE),
            max.z = max(Z, na.rm = TRUE),
            sd.x = sd(X, na.rm = TRUE),
            sd.y = sd(Y, na.rm =TRUE),
            sd.z = sd(Z, na.rm = TRUE),
            range.x = max.x - min.x,
            range.y = max.y - min.y,
            range.z = max.z - min.z
            )

```

```{r psd data mtry loop, eval = FALSE, include = FALSE}
set.seed(2807)
psd.model.mtry = c() # Do 'mtry' loop to identify best value for mtry
i=21
for (i in 1:21) {
  psd.model.mtry.train <- randomForest(behaviour ~ ., data = psd.data_train, ntree = 500, mtry = i, importance = TRUE)
  psd.predTest.mtry <- predict(psd.model.mtry.train, psd.data_test, type = "class")
  psd.model.mtry[i] = mean(psd.predTest.mtry == psd.data_test$behaviour)
}

psd.model.mtry.df = data.frame(mTry = c(1:21), Accuracy = c(psd.model.mtry))
psd.best.mtry = psd.model.mtry.df %>%
  arrange(desc(Accuracy)) %>%
  slice(1)

psd.best.mtry = psd.best.mtry$mTry

( ggplot(data = psd.model.mtry.df,aes(x = mTry, y = Accuracy )) + 
    geom_point(stat = "identity", size = 3,shape = "square",color = "red"))


```

```{r psd mtry plot, eval = FALSE, include = FALSE}
# To have two plots showcasing the difference in accuracy between both datasets
psd.model.mtry.df # psd
model.mtry.df # raw

psd.accuracy = psd.model.mtry.df$Accuracy
raw.accuracy = model.mtry.df$Accuracy
plot.mtry.df = as.data.frame(cbind(psd.accuracy, raw.accuracy))
plot.mtry.df$mTry = rep(1:21, each = 1)

(ggplot(data = plot.mtry.df, aes(x = mTry)) +
          geom_point(aes(y = psd.accuracy), color = "red", shape = 'square', size = 3) +
          geom_point(aes(y = raw.accuracy), color = "green", shape = 'square', size = 3)
)
```

```{r plotting raw data ROC curve, eval = FALSE, include = FALSE}
library(pROC)
roc = as.data.frame(predict(raw.model, rf.data_test, type = "prob"))
roc$predict = names(roc)[1:7][apply(roc[,1:7], 1, which.max)]
roc$observed = rf.data_test$behaviour
head(roc)



roc.run = roc(ifelse(roc$observed=="run", "run", "non-run"), 
              as.numeric(roc$run))
# plot(roc.run, col = 'gray60')


roc.eat = roc(ifelse(roc$observed=="eat", "eat", "non-eat"), 
              as.numeric(roc$eat))
# lines(roc.eat, col = 'green')

roc.forage = roc(ifelse(roc$observed=="forage", "forage", "non-forage"), as.numeric(roc$forage))
# lines(roc.forage, col = 'blue')

roc.walk = roc(ifelse(roc$observed=="walk", "walk", "non-walk"), 
               as.numeric(roc$walk))
# lines(roc.walk, col = 'red')

roc.lie = roc(ifelse(roc$observed=="lie", "lie", "non-lie"), 
              as.numeric(roc$lie))
#lines(roc.lie, col = 'brown')

roc.sit = roc(ifelse(roc$observed=="sit", "sit", "non-sit"), 
              as.numeric(roc$sit))
#lines(roc.sit, col = 'orange')

roc.stand = roc(ifelse(roc$observed=="stand", "stand", "non-stand"), as.numeric(roc$stand))
#lines(roc.stand, col = 'purple')


roc.list = list()
roc.list[[1]] = roc.run
roc.list[[2]] = roc.walk
roc.list[[3]] = roc.forage
roc.list[[4]] = roc.lie
roc.list[[5]] = roc.sit
roc.list[[6]] = roc.stand
roc.list[[7]] = roc.eat

g = ggroc(list(run = roc.run, walk = roc.walk, forage = roc.forage, lie = roc.lie,
               sit = roc.sit, stand = roc.stand, eat = roc.eat))
g

auc.roc = (auc(roc.run) + auc(roc.walk) + auc(roc.forage) + auc(roc.lie) + auc(roc.sit) + auc(roc.stand) + auc(roc.eat))/7
auc.roc
```

```{r rfcv, eval = FALSE, include = FALSE}
rfcv1 = as.data.frame(rf.data_train[,2:22])
rfcv2 = as.data.frame(rf.data_train[,1])
results = rfcv(rfcv1, rfcv2$behaviour, cv.fold = 5, scale = "log", step = 0.75)
with(results, plot(n.var, error.cv, log="x", type="o", lwd=2))
# Error rates are lowest when number of variables used range between 10 to 15? no different from just using mtry loop.
```